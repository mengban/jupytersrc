{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn import cross_validation\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is data_train <class 'numpy.ndarray'> (353999, 661)\n",
      "This is X_test <class 'numpy.ndarray'> (35400, 661)\n",
      "This is y_test <class 'numpy.ndarray'> (35400, 5)\n"
     ]
    }
   ],
   "source": [
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape) #_X  7352 128 9\n",
    "    shape[0] = batch_size      # 1500 128 9\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train) # step=1 \n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1  # from 0 +1\n",
    "    #n_values = 6\n",
    "    #print('The n_values is:',n_values)\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "def loaddata(Filename):\n",
    "    data = pd.read_csv(Filename,sep=',',header = None)\n",
    "    return np.array(data)\n",
    "# dataset\n",
    "data1 = loaddata(\"demo1.csv\")\n",
    "data2 = loaddata(\"demo2.csv\")\n",
    "data3 = loaddata(\"demo3.csv\")\n",
    "data4 = loaddata(\"demo4.csv\")\n",
    "data5 = loaddata(\"demo5.csv\")\n",
    "\n",
    "#data_train = np.vstack((data1[:len(data1)-1],data2[:len(data1)]))\n",
    "data_train = np.vstack((data_train,data3[:len(data3)-1]))\n",
    "data_train = np.vstack((data_train,data4[:len(data4)-1]))\n",
    "data_train = np.vstack((data_train,data5[:len(data5)-1]))\n",
    "\n",
    "print('This is data_train',type(data_train),data_train.shape)\n",
    "#label\n",
    "data1 = loaddata(\"label1.csv\")\n",
    "data2 = loaddata(\"label2.csv\")\n",
    "data3 = loaddata(\"label3.csv\")\n",
    "data4 = loaddata(\"label4.csv\")\n",
    "data5 = loaddata(\"label5.csv\")\n",
    "\n",
    "#label_train = np.vstack((data1[:len(data1)-1],data2[:len(data2)-1]))\n",
    "label_train = np.vstack((label_train,data3[:len(data3)-1]))\n",
    "label_train = np.vstack((label_train,data4[:len(data4)-1]))\n",
    "label_train = np.vstack((label_train,data5[:len(data5)-1]))\n",
    "#print(label_test[100:800])\n",
    "X_train,X_test,y_train_tmp,y_test_tmp=cross_validation.train_test_split(data_train,label_train,test_size=0.1)\n",
    "y_train = one_hot(y_train_tmp)\n",
    "y_test = one_hot(y_test_tmp)\n",
    "\n",
    "print('This is X_test',type(X_test),X_test.shape)\n",
    "print('This is y_test',type(y_test),y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568051\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  # Import data\n",
    "  # Create the model\n",
    "  x = tf.placeholder(tf.float32, [None, 661])\n",
    "  W = tf.Variable(tf.zeros([661, 5]))\n",
    "  b = tf.Variable(tf.zeros([5]))\n",
    "  y = tf.matmul(x, W) + b\n",
    "\n",
    "  # Define loss and optimizer\n",
    "  y_ = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "  train_step = tf.train.AdamOptimizer(0.12).minimize(cross_entropy)\n",
    "\n",
    "  sess = tf.InteractiveSession()\n",
    "  tf.global_variables_initializer().run()\n",
    "  # Train\n",
    "  for i in range(10000):\n",
    "    batch_xs = extract_batch_size(X_train,i,1000)\n",
    "    batch_ys = extract_batch_size(y_train,i,1000)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "  # Test trained model\n",
    "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  print(sess.run(accuracy, feed_dict={x:X_test,y_: y_test}))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/tensorflow/mnist/input_data\", one_hot=True)\n",
    "print(mnist.test.labels.shape)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) [[ 0.61220542]\n",
      " [ 0.89200234]\n",
      " [ 0.8860725 ]\n",
      " [ 0.14569721]\n",
      " [ 0.28599715]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.random.rand(5,1)\n",
    "print(a.shape,a,type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## record\n",
    "- 10000*1000 61%\n",
    "- all data 33.8%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
