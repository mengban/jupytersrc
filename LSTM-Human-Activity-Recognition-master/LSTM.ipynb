{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Human Activity Recognition\n",
    "\n",
    "Human activity recognition using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. Other research on the activity recognition dataset used mostly use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much did the data was preprocessed. \n",
    "\n",
    "## Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "## Details about input data\n",
    "\n",
    "I will be using an LSTM on the data to learn (as a cellphone attached on the waist) to recognise the type of activity that the user is doing. The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. \n",
    "\n",
    "## What is an RNN?\n",
    "\n",
    "As explained in [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), an RNN takes many input vectors to process them and output other vectors. It can be roughly pictured like in the image below, imagining each rectangle has a vectorial depth and other special hidden quirks in the image below. **In our case, the \"many to one\" architecture is used**: we accept time series of feature vectors (one vector per time step) to convert them to a probability vector at the output for classification. Note that a \"one to one\" architecture would be a standard feedforward neural network. \n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" />\n",
    "\n",
    "An LSTM is an improved RNN. It is more complex, but easier to train, avoiding what is called the vanishing gradient problem. \n",
    "\n",
    "\n",
    "## Results \n",
    "\n",
    "Scroll on! Nice visuals awaits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Includes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by downloading the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cadu/jupytersrc/LSTM-Human-Activity-Recognition-master\n",
      "data  LICENSE  LSTM_files  LSTM.ipynb  lstm.py\tREADME.md\n",
      "/home/cadu/jupytersrc/LSTM-Human-Activity-Recognition-master/data\n",
      "download_dataset.py  __MACOSX  source.txt  UCI HAR Dataset\n",
      "/home/cadu/jupytersrc/LSTM-Human-Activity-Recognition-master/data\n",
      "download_dataset.py  __MACOSX  source.txt  UCI HAR Dataset\n",
      "/home/cadu/jupytersrc/LSTM-Human-Activity-Recognition-master\n",
      "data  LICENSE  LSTM_files  LSTM.ipynb  lstm.py\tREADME.md\n",
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n"
     ]
    }
   ],
   "source": [
    "# Note: Linux bash commands start with a \"!\" inside those \"ipython notebook\" cells\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "!pwd && ls\n",
    "\n",
    "#!python download_dataset.py\n",
    "\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd && ls\n",
    "\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d72041c14d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_signals_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 7352 128 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_signals_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d72041c14d1a>\u001b[0m in \u001b[0;36mload_X\u001b[0;34m(X_signals_paths)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msignal_type_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_signals_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_type_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Read dataset from disk, dealing with text files' syntax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         X_signals.append(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt'"
     ]
    }
   ],
   "source": [
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths) \n",
    "print(X_train.shape,len(X_train))   # 7352 128 9\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1            #return y_-1 \n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "print('y_train.shape|len:',y_train.shape,len(y_train))   # 7352 1\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "n_values = int(np.max(y_train)) + 1\n",
    "print(n_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "The whole neural network's structure could be summarised by enumerating those parameters and the fact an LSTM is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size   _X(128,7352,9)\n",
    "    # Reshape to prepare input to hidden activation   \n",
    "    _X = tf.reshape(_X, [-1, n_input])                                 #_X (128*7352,9) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])  #  (128*7352,9)*(9,32)---->(128*7352,32)\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0)     #(_x ,128,0)--->128 *(7352,32)          /input _X is a batch_size(1500)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)                             /so all 7352 will be instead by 1500\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    #return a list of variables same\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # use a two-layers' lstm,the output of the first layer as the input of the second layer\n",
    "    # MultiRNNCell n_steps\n",
    "    # return a list of variables\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # **Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page**\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    #                                   [n_hidden ,n_classes]    [n_classes]\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']# the last output matmul\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape) #_X  7352 128 9\n",
    "    shape[0] = batch_size      # 1500 128 9\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train) # step=1 \n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    #print(n_values)\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get serious and build the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input]) # none 128 9\n",
    "y = tf.placeholder(tf.float32, [None, n_classes]) # none 6\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights  9 32\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0)) # 32 6\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),# 32\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))  #6\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(  #0.0015*sum\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() #fan hui xu yao xun lian de bianliang liebiao\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray, now train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1500:   Batch Loss = 3.337801, Accuracy = 0.07599999755620956\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.7984023094177246, Accuracy = 0.1469290852546692\n",
      "Training iter #30000:   Batch Loss = 1.285067, Accuracy = 0.6893333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3231678009033203, Accuracy = 0.7074991464614868\n",
      "Training iter #60000:   Batch Loss = 1.054842, Accuracy = 0.8159999847412109\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.226508617401123, Accuracy = 0.7550050616264343\n",
      "Training iter #90000:   Batch Loss = 1.018993, Accuracy = 0.831333339214325\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1353247165679932, Accuracy = 0.7987784147262573\n",
      "Training iter #120000:   Batch Loss = 0.776645, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.086511492729187, Accuracy = 0.8364438414573669\n",
      "Training iter #150000:   Batch Loss = 0.692016, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9551142454147339, Accuracy = 0.8696979880332947\n",
      "Training iter #180000:   Batch Loss = 0.740295, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8873193264007568, Accuracy = 0.8798778653144836\n",
      "Training iter #210000:   Batch Loss = 0.724676, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9395864009857178, Accuracy = 0.8761452436447144\n",
      "Training iter #240000:   Batch Loss = 0.630965, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9582488536834717, Accuracy = 0.8683406710624695\n",
      "Training iter #270000:   Batch Loss = 0.703392, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0533195734024048, Accuracy = 0.8171021342277527\n",
      "Training iter #300000:   Batch Loss = 0.615294, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8999030590057373, Accuracy = 0.8730912804603577\n",
      "Training iter #330000:   Batch Loss = 0.670255, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9697469472885132, Accuracy = 0.8639293909072876\n",
      "Training iter #360000:   Batch Loss = 0.641441, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9096623659133911, Accuracy = 0.8778418898582458\n",
      "Training iter #390000:   Batch Loss = 0.645415, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8291041254997253, Accuracy = 0.8951476216316223\n",
      "Training iter #420000:   Batch Loss = 0.581338, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8733773231506348, Accuracy = 0.8910756707191467\n",
      "Training iter #450000:   Batch Loss = 0.557885, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8809581398963928, Accuracy = 0.8873430490493774\n",
      "Training iter #480000:   Batch Loss = 0.578467, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.898248553276062, Accuracy = 0.8842890858650208\n",
      "Training iter #510000:   Batch Loss = 0.577214, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9413325786590576, Accuracy = 0.8646080493927002\n",
      "Training iter #540000:   Batch Loss = 0.652209, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8794372081756592, Accuracy = 0.8574821949005127\n",
      "Training iter #570000:   Batch Loss = 0.583837, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7797032594680786, Accuracy = 0.8971835970878601\n",
      "Training iter #600000:   Batch Loss = 0.543415, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7933274507522583, Accuracy = 0.9009161591529846\n",
      "Training iter #630000:   Batch Loss = 0.463742, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8310199975967407, Accuracy = 0.8907363414764404\n",
      "Training iter #660000:   Batch Loss = 0.475595, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8802599906921387, Accuracy = 0.8758059144020081\n",
      "Training iter #690000:   Batch Loss = 0.493287, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7857738733291626, Accuracy = 0.8951476216316223\n",
      "Training iter #720000:   Batch Loss = 0.507025, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8332555294036865, Accuracy = 0.8751272559165955\n",
      "Training iter #750000:   Batch Loss = 0.504589, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8416509628295898, Accuracy = 0.8883610367774963\n",
      "Training iter #780000:   Batch Loss = 0.436584, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8692203760147095, Accuracy = 0.8676620125770569\n",
      "Training iter #810000:   Batch Loss = 0.444110, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7671473622322083, Accuracy = 0.8764845728874207\n",
      "Training iter #840000:   Batch Loss = 0.480221, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8356457948684692, Accuracy = 0.8785205483436584\n",
      "Training iter #870000:   Batch Loss = 0.533090, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7640923261642456, Accuracy = 0.8771632313728333\n",
      "Training iter #900000:   Batch Loss = 0.414040, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7572299242019653, Accuracy = 0.8832710981369019\n",
      "Training iter #930000:   Batch Loss = 0.495119, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7660101056098938, Accuracy = 0.884628415107727\n",
      "Training iter #960000:   Batch Loss = 0.448820, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8231589794158936, Accuracy = 0.8785205483436584\n",
      "Training iter #990000:   Batch Loss = 0.376546, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8364289402961731, Accuracy = 0.8747879266738892\n",
      "Training iter #1020000:   Batch Loss = 0.372591, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8504048585891724, Accuracy = 0.8622328042984009\n",
      "Training iter #1050000:   Batch Loss = 0.364665, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.772464394569397, Accuracy = 0.8761452436447144\n",
      "Training iter #1080000:   Batch Loss = 0.389749, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7676854729652405, Accuracy = 0.8876823782920837\n",
      "Training iter #1110000:   Batch Loss = 0.408721, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8065794706344604, Accuracy = 0.8761452436447144\n",
      "Training iter #1140000:   Batch Loss = 0.395472, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8188190460205078, Accuracy = 0.8758059144020081\n",
      "Training iter #1170000:   Batch Loss = 0.383047, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8102482557296753, Accuracy = 0.8649473786354065\n",
      "Training iter #1200000:   Batch Loss = 0.424517, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7250682711601257, Accuracy = 0.8751272559165955\n",
      "Training iter #1230000:   Batch Loss = 0.388684, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.647134006023407, Accuracy = 0.8853070735931396\n",
      "Training iter #1260000:   Batch Loss = 0.344659, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7200922966003418, Accuracy = 0.8849677443504333\n",
      "Training iter #1290000:   Batch Loss = 0.367466, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7182868719100952, Accuracy = 0.8887003660202026\n",
      "Training iter #1320000:   Batch Loss = 0.376077, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7520571947097778, Accuracy = 0.8805565237998962\n",
      "Training iter #1350000:   Batch Loss = 0.336592, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7912757396697998, Accuracy = 0.8727519512176514\n",
      "Training iter #1380000:   Batch Loss = 0.310045, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7648745775222778, Accuracy = 0.8615541458129883\n",
      "Training iter #1410000:   Batch Loss = 0.296851, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7566025853157043, Accuracy = 0.8754665851593018\n",
      "Training iter #1440000:   Batch Loss = 0.312596, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7577989101409912, Accuracy = 0.8747879266738892\n",
      "Training iter #1470000:   Batch Loss = 0.348127, Accuracy = 0.9626666903495789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7526392340660095, Accuracy = 0.8710553050041199\n",
      "Training iter #1500000:   Batch Loss = 0.621023, Accuracy = 0.8606666922569275\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6946989297866821, Accuracy = 0.8561248779296875\n",
      "Training iter #1530000:   Batch Loss = 0.363038, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6487736105918884, Accuracy = 0.8764845728874207\n",
      "Training iter #1560000:   Batch Loss = 0.324067, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6474399566650391, Accuracy = 0.8971835970878601\n",
      "Training iter #1590000:   Batch Loss = 0.351760, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6413187980651855, Accuracy = 0.8978622555732727\n",
      "Training iter #1620000:   Batch Loss = 0.301781, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6632908582687378, Accuracy = 0.8829317688941956\n",
      "Training iter #1650000:   Batch Loss = 0.332721, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6725432872772217, Accuracy = 0.8785205483436584\n",
      "Training iter #1680000:   Batch Loss = 0.322776, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7052223682403564, Accuracy = 0.8781812191009521\n",
      "Training iter #1710000:   Batch Loss = 0.537349, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7758128643035889, Accuracy = 0.8574821949005127\n",
      "Training iter #1740000:   Batch Loss = 0.292746, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5756102204322815, Accuracy = 0.8893790245056152\n",
      "Training iter #1770000:   Batch Loss = 0.291756, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5494424700737, Accuracy = 0.8992195725440979\n",
      "Training iter #1800000:   Batch Loss = 0.277717, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6324294805526733, Accuracy = 0.8873430490493774\n",
      "Training iter #1830000:   Batch Loss = 0.347434, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5997028350830078, Accuracy = 0.8863250613212585\n",
      "Training iter #1860000:   Batch Loss = 0.336814, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6097524166107178, Accuracy = 0.8897183537483215\n",
      "Training iter #1890000:   Batch Loss = 0.286060, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.590177595615387, Accuracy = 0.8961656093597412\n",
      "Training iter #1920000:   Batch Loss = 0.271489, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6346235275268555, Accuracy = 0.8965049386024475\n",
      "Training iter #1950000:   Batch Loss = 0.284105, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6476844549179077, Accuracy = 0.8873430490493774\n",
      "Training iter #1980000:   Batch Loss = 0.323802, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5933577418327332, Accuracy = 0.8937903046607971\n",
      "Training iter #2010000:   Batch Loss = 0.272773, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5859086513519287, Accuracy = 0.8890396952629089\n",
      "Training iter #2040000:   Batch Loss = 0.316430, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6406722664833069, Accuracy = 0.885646402835846\n",
      "Training iter #2070000:   Batch Loss = 0.290967, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6198936700820923, Accuracy = 0.8825924396514893\n",
      "Training iter #2100000:   Batch Loss = 0.224946, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6259818077087402, Accuracy = 0.885646402835846\n",
      "Training iter #2130000:   Batch Loss = 0.231189, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6265742778778076, Accuracy = 0.8870037198066711\n",
      "Training iter #2160000:   Batch Loss = 0.241916, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7045940160751343, Accuracy = 0.8639293909072876\n",
      "Training iter #2190000:   Batch Loss = 0.310998, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.80790114402771, Accuracy = 0.8194774389266968\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 0.6760392785072327, Accuracy = 0.8754665851593018\n"
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:    # 1*1500 <7352*300\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)# (1500,128,9) step++\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))#[[100000],,,,[000001]]\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training is good, but having visual insight is even better:\n",
    "\n",
    "Okay, let's plot this simply in the notebook for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-4fd7e23bae0b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-4fd7e23bae0b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    'weight' : 'bold'\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# (Inline plots: )\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\n",
    "    'family' : 'Bitstream Vera Sans',w\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "plt.plot(indep_test_axis, np.array(test_losses),     \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the multi-class confusion matrix and metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Outstandingly, **the final accuracy is of 91%**! And it can peak to values such as 92.73%, at some moments of luck during the training, depending on how the neural network's weights got initialized at the start of the training, randomly. \n",
    "\n",
    "This means that the neural networks is almost always able to correctly identify the movement type! Remember, the phone is attached on the waist and each series to classify has just a 128 sample window of two internal sensors (a.k.a. 2.56 seconds at 50 FPS), so those predictions are extremely accurate.\n",
    "\n",
    "I specially did not expect such good results for guessing between \"SITTING\" and \"STANDING\". Those are seemingly almost the same thing from the point of view of a device placed at waist level according to how the dataset was gathered. Thought, it is still possible to see a little cluster on the matrix between those classes, which drifts away from the identity. This is great.\n",
    "\n",
    "It is also possible to see that there was a slight difficulty in doing the difference between \"WALKING\", \"WALKING_UPSTAIRS\" and \"WALKING_DOWNSTAIRS\". Obviously, those activities are quite similar in terms of movements. \n",
    "\n",
    "I also tried my code without the gyroscope, using only the two 3D accelerometer's features (and not changing the training hyperparameters), and got an accuracy of 87%. In general, gyroscopes consumes more power than accelerometers, so it is preferable to turn them off. \n",
    "\n",
    "\n",
    "## Improvements\n",
    "\n",
    "In [another open-source repository of mine](https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs), the accuracy is pushed up to 94% using a special deep LSTM architecture which combines the concepts of bidirectional RNNs, residual connections and stacked cells. This architecture is also tested on another similar activity dataset. It resembles to the architecture used in \"[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)\" without an attention mechanism and with just the encoder part - still as a \"many to one\" architecture which is adapted to the Human Activity Recognition (HAR) problem.\n",
    "\n",
    "If you want to learn more about deep learning, I have also built a list of the learning ressources for deep learning which have revealed to be the most useful to me [here](https://github.com/guillaume-chevalier/awesome-deep-learning-resources). You could as well learn to [learn to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf) (not for the faint of heart). Ok, I pushed the joke deep enough... \n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) can be found on the UCI Machine Learning Repository. \n",
    "\n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
    "\n",
    "To cite my work, point to the URL of the GitHub repository: \n",
    "> Guillaume Chevalier, LSTMs for Human Activity Recognition, 2016\n",
    "> https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n",
    "My code is available under the [MIT License](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/blob/master/LICENSE). \n",
    "\n",
    "## Connect with me\n",
    "\n",
    "- https://ca.linkedin.com/in/chevalierg \n",
    "- https://twitter.com/guillaume_che\n",
    "- https://github.com/guillaume-chevalier/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's convert this notebook to a README for the GitHub project's title page:\n",
    "!jupyter nbconvert --to markdown LSTM.ipynb\n",
    "!mv LSTM.md README.md"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
